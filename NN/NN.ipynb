{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import linalg\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from sklearn import preprocessing, datasets\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, adjusted_mutual_info_score, silhouette_samples, calinski_harabaz_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "from sklearn import mixture\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.decomposition import PCA, FastICA, NMF, IncrementalPCA, FactorAnalysis, SparsePCA, TruncatedSVD\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=black size=5>Load Data</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_data = datasets.load_breast_cancer()\n",
    "# sklearn_data = datasets.load_digits()\n",
    "# print(sklearn_data.DESCR)\n",
    "\n",
    "x, y = sklearn_data.data, sklearn_data.target\n",
    "x = preprocessing.scale(x) \n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.4, random_state=1997)\n",
    "\n",
    "# print(sklearn_data.data.shape)\n",
    "# print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_net(X_train,y_train,X_test,y_test,learning_rate,plotting):\n",
    "    clf = MLPClassifier(activation='tanh',alpha=1e-03,batch_size='auto',learning_rate='adaptive',learning_rate_init=learning_rate,solver='adam')\n",
    "    st = time.time()\n",
    "    clf.fit(X_train,y_train)\n",
    "    end = time.time()\n",
    "    train_time = end-st\n",
    "    stt = time.time()\n",
    "    y_pred = clf.predict(X_test)\n",
    "    endt = time.time()\n",
    "    y_test = np.array(y_test)\n",
    "    y_pred = np.array(y_pred)\n",
    "    title = 'Confusion Matrix with Neural Network'\n",
    "    clf_name = 'neural_net'\n",
    "    if plotting == True:\n",
    "        plot_learning_curve(clf,'Learning Curve for Neural Network', X_train, y_train, (0.7, 1.01), n_jobs=4)\n",
    "        plot_validation_curve(X_train,y_train,clf,clf_name)\n",
    "        confusion(y_test,y_pred,title)\n",
    "\n",
    "    print('Accuracy for Neural Network is ' + str(accuracy_score(y_test,y_pred)))\n",
    "    print('Training time for Neural Network: ' + str(train_time) + ' seconds')\n",
    "    print('Testing time for Neural Network: ' + str(endt-stt) + ' seconds')\n",
    "    print()\n",
    "\n",
    "def tuned_neural_net(X_train,y_train,X_test,y_test,learning_rate,plotting):\n",
    "    clf = MLPClassifier(activation='relu',max_iter=5000,alpha=1e-05,batch_size='auto',learning_rate='adaptive',learning_rate_init=learning_rate,solver='adam')\n",
    "    alpha_range = [1e-05,1e-04,1e-03,1e-02,1e-01,1,2,3]\n",
    "    lr_range = [0.00001,0.0001,0.001,0.01,0.1,0.5,0.8,1]\n",
    "    params = {'alpha' : alpha_range, 'learning_rate_init' : lr_range, 'activation': ['relu','tanh']}\n",
    "    clf = GridSearchCV(clf, param_grid=params, cv=5)\n",
    "    st = time.time()\n",
    "    clf.fit(X_train,y_train)\n",
    "    end = time.time()\n",
    "    train_time = end-st\n",
    "    stt = time.time()\n",
    "    y_pred = clf.predict(X_test)\n",
    "    endt = time.time()\n",
    "    y_test = np.array(y_test)\n",
    "    y_pred = np.array(y_pred)\n",
    "    title = 'Confusion Matrix with Tuned Neural Network'\n",
    "    clf_name = 'neural_net'\n",
    "    if plotting == True:\n",
    "        plot_learning_curve(clf,'Learning Curve for Neural Network', X_train, y_train, (0.7, 1.01), n_jobs=4)\n",
    "        confusion(y_test,y_pred,title)\n",
    "        clf_best = MLPClassifier(hidden_layer_sizes=(5, 2), random_state = 3169, max_iter = 1)\n",
    "        clf_best.set_params(alpha=clf.best_params_['alpha'], learning_rate_init=clf.best_params_['learning_rate_init'])\n",
    "        num_epochs = 1000\n",
    "        train_loss = np.zeros(num_epochs)\n",
    "        train_scores = np.zeros(num_epochs)\n",
    "        val_scores = np.zeros(num_epochs)\n",
    "        # Split training set into training and validation\n",
    "        X_train_, X_val, y_train_, y_val = train_test_split(X_train, y_train, test_size=0.4, random_state=3169)\n",
    "        for i in range(num_epochs):\n",
    "            clf_best.fit(X_train_, y_train_)\n",
    "            train_loss[i] = clf_best.loss_\n",
    "            train_scores[i] = accuracy_score(y_train_, clf_best.predict(X_train_))\n",
    "        range_loss = np.arange(num_epochs) + 1\n",
    "        plt.figure()\n",
    "        plt.plot(range_loss, train_loss)\n",
    "        plt.title('Training loss curve for neural network')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "    print('Accuracy for Tuned Neural Network is ' + str(accuracy_score(y_test,y_pred)))\n",
    "    print('Training time for Tuned NN: ' + str(train_time) + ' seconds')\n",
    "    print('Testing time for Tuned NN: ' + str(endt-stt) + ' seconds')\n",
    "    print('Best parameters: ' + str(clf.best_params_))\n",
    "    print()\n",
    "    return clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def K_Means_silhouette_analysis(X,y):\n",
    "#     range_n_clusters = [3, 7, 10, 15, 17, 19, 20]\n",
    "\n",
    "    range_n_clusters = [10]\n",
    "\n",
    "    for n_clusters in range_n_clusters:\n",
    "        # Create a subplot with 1 row and 2 columns\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "        fig.set_size_inches(18, 7)\n",
    "\n",
    "        # The 1st subplot is the silhouette plot\n",
    "        # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "        # lie within [-0.1, 1]\n",
    "        ax1.set_xlim([-0.1, 1])\n",
    "        # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "        # plots of individual clusters, to demarcate them clearly.\n",
    "        ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "        # Initialize the clusterer with n_clusters value and a random generator\n",
    "        # seed of 10 for reproducibility.\n",
    "        clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "        cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "        # The silhouette_score gives the average value for all the samples.\n",
    "        # This gives a perspective into the density and separation of the formed\n",
    "        # clusters\n",
    "        silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "        print(\"For n_clusters =\", n_clusters,\n",
    "              \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "        # Compute the silhouette scores for each sample\n",
    "        sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "        y_lower = 10\n",
    "        for i in range(n_clusters):\n",
    "            # Aggregate the silhouette scores for samples belonging to\n",
    "            # cluster i, and sort them\n",
    "            ith_cluster_silhouette_values = \\\n",
    "                sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "            ith_cluster_silhouette_values.sort()\n",
    "\n",
    "            size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "            y_upper = y_lower + size_cluster_i\n",
    "\n",
    "            color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "            ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                              0, ith_cluster_silhouette_values,\n",
    "                              facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "            # Label the silhouette plots with their cluster numbers at the middle\n",
    "            ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "            # Compute the new y_lower for next plot\n",
    "            y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "        ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "        ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "        ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "        # The vertical line for average silhouette score of all the values\n",
    "        ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "        ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "        ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "        # 2nd Plot showing the actual clusters formed\n",
    "        colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "        ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                    c=colors, edgecolor='k')\n",
    "\n",
    "        # Labeling the clusters\n",
    "        centers = clusterer.cluster_centers_\n",
    "        # Draw white circles at cluster centers\n",
    "        ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                    c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "        for i, c in enumerate(centers):\n",
    "            ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "                        s=50, edgecolor='k')\n",
    "\n",
    "\n",
    "        ax2.set_title(\"The visualization of the clustered data.\")\n",
    "        ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "        ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "        plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                      \"with n_clusters = %d\" % n_clusters),\n",
    "                     fontsize=14, fontweight='bold')\n",
    "\n",
    "        plt.show()\n",
    "# K_Means_silhouette_analysis(x,y)\n",
    "\n",
    "def K_Means_Results(best_n,X,y):\n",
    "    kmeans = KMeans(n_clusters=best_n, random_state=1997)\n",
    "    kmeans.fit(X)\n",
    "    print('K-Means Inertia: ', kmeans.inertia_)\n",
    "    silh_result = silhouette_score(X, kmeans.labels_)\n",
    "    print('K-Means Silhouette score: ', silh_result)\n",
    "    AMI = adjusted_mutual_info_score(y, kmeans.labels_)\n",
    "    print('K-Means Adjusted Mutual Information (AMI) score: ', AMI)\n",
    "    print()\n",
    "    return kmeans.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmm_em(X,y):\n",
    "    lowest_bic = np.infty\n",
    "    bic = []\n",
    "    n_components_range = range(1, 20)\n",
    "    cv_types = ['spherical', 'tied', 'diag', 'full']\n",
    "    for cv_type in cv_types:\n",
    "        for n_components in n_components_range:\n",
    "            # Fit a Gaussian mixture with EM\n",
    "            gmm = mixture.GaussianMixture(n_components=n_components,\n",
    "                                          covariance_type='full')\n",
    "            gmm.fit(X)\n",
    "            bic.append(gmm.bic(X))\n",
    "            if bic[-1] < lowest_bic:\n",
    "                lowest_bic = bic[-1]\n",
    "                best_gmm = gmm\n",
    "\n",
    "    bic = np.array(bic)\n",
    "    color_iter = itertools.cycle(['navy', 'turquoise', 'cornflowerblue',\n",
    "                                  'darkorange', 'green'])\n",
    "    clf = best_gmm\n",
    "    bars = []\n",
    "\n",
    "    # Plot the BIC scores\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    spl = plt.subplot(2, 1, 1)\n",
    "    for i, (cv_type, color) in enumerate(zip(cv_types, color_iter)):\n",
    "        xpos = np.array(n_components_range) + .2 * (i - 2)\n",
    "        bars.append(plt.bar(xpos, bic[i * len(n_components_range):\n",
    "                                      (i + 1) * len(n_components_range)],\n",
    "                            width=.2, color=color))\n",
    "    plt.xticks(n_components_range)\n",
    "    plt.ylim([bic.min() * 1.01 - .01 * bic.max(), bic.max()])\n",
    "    plt.title('BIC score per model')\n",
    "    xpos = np.mod(bic.argmin(), len(n_components_range)) + .65 +\\\n",
    "        .2 * np.floor(bic.argmin() / len(n_components_range))\n",
    "    best_num = np.mod(bic.argmin(), len(n_components_range)) + 1\n",
    "    print(best_num)\n",
    "    plt.text(xpos, bic.min() * 0.97 + .03 * bic.max(), '*', fontsize=14)\n",
    "    spl.set_xlabel('Number of components')\n",
    "    spl.legend([b[0] for b in bars], cv_types)\n",
    "\n",
    "    # Plot the winner\n",
    "    splot = plt.subplot(2, 1, 2)\n",
    "    Y_ = clf.predict(X)\n",
    "    print(clf.covariances_.shape)\n",
    "    for i, (mean, cov, color) in enumerate(zip(clf.means_, clf.covariances_,\n",
    "                                               color_iter)):\n",
    "        print(cov.shape)\n",
    "        v, w = linalg.eigh(cov)\n",
    "        if not np.any(Y_ == i):\n",
    "            continue\n",
    "#         plt.scatter(X[Y_ == i, 18], X[Y_ == i, 45], .8, color=color)\n",
    "        plt.scatter(X[Y_ == i, 0], X[Y_ == i, 1], .8, color=color)\n",
    "\n",
    "\n",
    "        # Plot an ellipse to show the Gaussian component\n",
    "        angle = np.arctan2(w[0][1], w[0][0])\n",
    "        angle = 180. * angle / np.pi  # convert to degrees\n",
    "        v = 2. * np.sqrt(2.) * np.sqrt(v)\n",
    "        ell = mpl.patches.Ellipse(mean, v[0], v[1], 180. + angle, color=color)\n",
    "        ell.set_clip_box(splot.bbox)\n",
    "        ell.set_alpha(.5)\n",
    "        splot.add_artist(ell)\n",
    "\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "\n",
    "    plt.title('Selected GMM: full model,' + str(best_num) + ' components')\n",
    "\n",
    "    plt.subplots_adjust(hspace=.35, bottom=.02)\n",
    "    plt.show()\n",
    "    return best_num\n",
    "# gmm_em(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmm_results(best_n,X,y):\n",
    "    gmm_best = GaussianMixture(n_components=best_n, random_state=1997)\n",
    "    gmm_best.fit(X)\n",
    "    gmm_labels = gmm_best.predict(X)\n",
    "\n",
    "    print('GMM BIC: ', gmm_best.bic(X))\n",
    "    score_gmm = silhouette_score(X, gmm_labels)\n",
    "    print('GMM Silhouette score: ', score_gmm)\n",
    "    AMI_gmm = adjusted_mutual_info_score(y, gmm_labels)\n",
    "    print('GMM Adjusted Mutual Information (AMI) score: ', AMI_gmm)\n",
    "    print()\n",
    "    return gmm_best.predict_proba(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=black size=5>NN+DR</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_n = 2\n",
    "\n",
    "best_X_pca = PCA(n_components = best_n).fit_transform(x)\n",
    "print('Neural Network with PCA Data: ')\n",
    "print()\n",
    "X_train, X_test, y_train, y_test = train_test_split(best_X_pca, y, test_size=0.4, random_state=1997)\n",
    "# neural_net(X_train,y_train,X_test,y_test,learning_rate=0.1,plotting=False)\n",
    "st = time.time()\n",
    "neural_net(X_train,y_train,X_test,y_test,learning_rate=0.1,plotting=False)\n",
    "end = time.time()\n",
    "print('PCA Time:' + str(end-st))\n",
    "# best_nn = tuned_neural_net(X_train,y_train,X_test,y_test,learning_rate=0.0001,plotting=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_X_ica = FastICA(n_components = best_n).fit_transform(x)\n",
    "print('Neural Network with ICA Data: ')\n",
    "print()\n",
    "X_train, X_test, y_train, y_test = train_test_split(best_X_ica, y, test_size=0.4, random_state=1997)\n",
    "st = time.time()\n",
    "neural_net(X_train,y_train,X_test,y_test,learning_rate=0.1,plotting=False)\n",
    "end = time.time()\n",
    "print('ICA Time:' + str(end-st))\n",
    "# neural_net(X_train,y_train,X_test,y_test,learning_rate=0.1,plotting=False)\n",
    "# best_nn = tuned_neural_net(X_train,y_train,X_test,y_test,learning_rate=0.0001,plotting=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_X_rp = GaussianRandomProjection(n_components = best_n).fit_transform(x)\n",
    "print('Neural Network with Randomly Projected Data: ')\n",
    "print()\n",
    "X_train, X_test, y_train, y_test = train_test_split(best_X_rp, y, test_size=0.4, random_state=1997)\n",
    "st = time.time()\n",
    "neural_net(X_train,y_train,X_test,y_test,learning_rate=0.1,plotting=False)\n",
    "end = time.time()\n",
    "print('RP Time:' + str(end-st))\n",
    "# neural_net(X_train,y_train,X_test,y_test,learning_rate=0.1,plotting=False)\n",
    "# best_nn = tuned_neural_net(X_train,y_train,X_test,y_test,learning_rate=0.0001,plotting=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_X_fa = FactorAnalysis(n_components = best_n).fit_transform(x)\n",
    "print('Neural Network with Factor Analysed Data: ')\n",
    "print()\n",
    "X_train, X_test, y_train, y_test = train_test_split(best_X_fa, y, test_size=0.4, random_state=1997)\n",
    "st = time.time()\n",
    "neural_net(X_train,y_train,X_test,y_test,learning_rate=0.1,plotting=False)\n",
    "end = time.time()\n",
    "print('FA Time:' + str(end-st))\n",
    "# neural_net(X_train,y_train,X_test,y_test,learning_rate=0.1,plotting=False)\n",
    "# best_nn = tuned_neural_net(X_train,y_train,X_test,y_test,learning_rate=0.0001,plotting=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=black size=5>NN+DR+CA</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('### RUNNING NEURAL NETWORK WITH CLUSTERED DATA AS FEATURES')\n",
    "\n",
    "print('### PCA')\n",
    "K_Means_silhouette_analysis(best_X_pca,y)\n",
    "K_Clustered_X = K_Means_Results(10,best_X_pca,y)\n",
    "# best_gmm_n = gmm_em(best_X_pca,y)\n",
    "GMM_Clustered_X = gmm_results(10,best_X_pca,y)\n",
    "\n",
    "\n",
    "print('K-Means Clustering & Neural Network: ')\n",
    "X_train, X_test, y_train, y_test = train_test_split(K_Clustered_X, y, test_size=0.4, random_state=1997)\n",
    "neural_net(X_train,y_train,X_test,y_test,learning_rate=0.1,plotting=False)\n",
    "\n",
    "best_nn = tuned_neural_net(X_train,y_train,X_test,y_test,learning_rate=0.0001,plotting=False)\n",
    "print()\n",
    "\n",
    "\n",
    "print('GMM & Neural Network: ')\n",
    "X_train, X_test, y_train, y_test = train_test_split(GMM_Clustered_X, y, test_size=0.4, random_state=1997)\n",
    "neural_net(X_train,y_train,X_test,y_test,learning_rate=0.1,plotting=False)\n",
    "\n",
    "best_nn = tuned_neural_net(X_train,y_train,X_test,y_test,learning_rate=0.0001,plotting=False)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('### ICA')\n",
    "K_Means_silhouette_analysis(best_X_ica,y)\n",
    "K_Clustered_X = K_Means_Results(10,x,y)\n",
    "# best_gmm_n = gmm_em(best_X_ica,y)\n",
    "GMM_Clustered_X = gmm_results(10,x,y)\n",
    "\n",
    "print('K-Means Clustering & Neural Network: ')\n",
    "X_train, X_test, y_train, y_test = train_test_split(K_Clustered_X, y, test_size=0.4, random_state=1997)\n",
    "neural_net(X_train,y_train,X_test,y_test,learning_rate=0.1,plotting=False)\n",
    "\n",
    "# best_nn = tuned_neural_net(X_train,y_train,X_test,y_test,learning_rate=0.0001,plotting=False)\n",
    "print()\n",
    "\n",
    "print('GMM & Neural Network: ')\n",
    "X_train, X_test, y_train, y_test = train_test_split(GMM_Clustered_X, y, test_size=0.4, random_state=1997)\n",
    "neural_net(X_train,y_train,X_test,y_test,learning_rate=0.1,plotting=False)\n",
    "\n",
    "# best_nn = tuned_neural_net(X_train,y_train,X_test,y_test,learning_rate=0.0001,plotting=False)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('### RP')\n",
    "K_Means_silhouette_analysis(best_X_rp,y)\n",
    "K_Clustered_X = K_Means_Results(10,x,y)\n",
    "# best_gmm_n = gmm_em(best_X_rp,y)\n",
    "GMM_Clustered_X = gmm_results(10,x,y)\n",
    "\n",
    "print('K-Means Clustering & Neural Network: ')\n",
    "X_train, X_test, y_train, y_test = train_test_split(K_Clustered_X, y, test_size=0.4, random_state=1997)\n",
    "neural_net(X_train,y_train,X_test,y_test,learning_rate=0.1,plotting=False)\n",
    "\n",
    "# best_nn = tuned_neural_net(X_train,y_train,X_test,y_test,learning_rate=0.0001,plotting=False)\n",
    "print()\n",
    "\n",
    "print('GMM & Neural Network: ')\n",
    "X_train, X_test, y_train, y_test = train_test_split(GMM_Clustered_X, y, test_size=0.4, random_state=1997)\n",
    "neural_net(X_train,y_train,X_test,y_test,learning_rate=0.1,plotting=False)\n",
    "\n",
    "# best_nn = tuned_neural_net(X_train,y_train,X_test,y_test,learning_rate=0.0001,plotting=False)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('### FA')\n",
    "K_Means_silhouette_analysis(best_X_fa,y)\n",
    "K_Clustered_X = K_Means_Results(10,x,y)\n",
    "# best_gmm_n = gmm_em(best_X_fa,y)\n",
    "GMM_Clustered_X = gmm_results(10,x,y)\n",
    "\n",
    "print('K-Means Clustering & Neural Network: ')\n",
    "X_train, X_test, y_train, y_test = train_test_split(K_Clustered_X, y, test_size=0.4, random_state=1997)\n",
    "neural_net(X_train,y_train,X_test,y_test,learning_rate=0.1,plotting=False)\n",
    "\n",
    "# best_nn = tuned_neural_net(X_train,y_train,X_test,y_test,learning_rate=0.0001,plotting=False)\n",
    "print()\n",
    "\n",
    "print('GMM & Neural Network: ')\n",
    "X_train, X_test, y_train, y_test = train_test_split(GMM_Clustered_X, y, test_size=0.4, random_state=1997)\n",
    "neural_net(X_train,y_train,X_test,y_test,learning_rate=0.1,plotting=False)\n",
    "\n",
    "# best_nn = tuned_neural_net(X_train,y_train,X_test,y_test,learning_rate=0.0001,plotting=False)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### O\n",
      "K-Means Inertia:  8555.100550649087\n",
      "K-Means Silhouette score:  0.16093390221330767\n",
      "K-Means Adjusted Mutual Information (AMI) score:  0.29229696479367734\n",
      "\n",
      "GMM BIC:  6094.040119869131\n",
      "GMM Silhouette score:  0.31448870991364997\n",
      "GMM Adjusted Mutual Information (AMI) score:  0.6597028491344276\n",
      "\n",
      "K-Means Clustering & Neural Network: \n",
      "Accuracy for Neural Network is 0.9780701754385965\n",
      "Training time for Neural Network: 0.11728143692016602 seconds\n",
      "Testing time for Neural Network: 0.0 seconds\n",
      "\n",
      "\n",
      "GMM & Neural Network: \n",
      "Accuracy for Neural Network is 0.9824561403508771\n",
      "Training time for Neural Network: 0.14056921005249023 seconds\n",
      "Testing time for Neural Network: 0.0 seconds\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('### O')\n",
    "# K_Means_silhouette_analysis(x,y)\n",
    "K_Clustered_X = K_Means_Results(5,x,y)\n",
    "# best_gmm_n = gmm_em(x,y)\n",
    "GMM_Clustered_X = gmm_results(2,x,y)\n",
    "\n",
    "\n",
    "print('K-Means Clustering & Neural Network: ')\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.4, random_state=1997)\n",
    "neural_net(X_train,y_train,X_test,y_test,learning_rate=0.1,plotting=False)\n",
    "\n",
    "# best_nn = tuned_neural_net(X_train,y_train,X_test,y_test,learning_rate=0.0001,plotting=False)\n",
    "print()\n",
    "\n",
    "\n",
    "print('GMM & Neural Network: ')\n",
    "# X_train, X_test, y_train, y_test = train_test_split(GMM_Clustered_X, y, test_size=0.4, random_state=1997)\n",
    "neural_net(X_train,y_train,X_test,y_test,learning_rate=0.1,plotting=False)\n",
    "\n",
    "# best_nn = tuned_neural_net(X_train,y_train,X_test,y_test,learning_rate=0.0001,plotting=False)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Neural Network is 0.9780701754385965\n",
      "Training time for Neural Network: 0.015623807907104492 seconds\n",
      "Testing time for Neural Network: 0.0 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "neural_net(X_train,y_train,X_test,y_test,learning_rate=0.1,plotting=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
